{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libs\n!pip install -U segmentation-models-pytorch --user ","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:30.572Z","iopub.execute_input":"2021-11-21T04:42:30.572584Z","iopub.status.idle":"2021-11-21T04:42:43.761531Z","shell.execute_reply.started":"2021-11-21T04:42:30.572501Z","shell.execute_reply":"2021-11-21T04:42:43.760635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python --version","metadata":{"execution":{"iopub.status.busy":"2021-11-21T06:59:42.058255Z","iopub.execute_input":"2021-11-21T06:59:42.059083Z","iopub.status.idle":"2021-11-21T06:59:42.914281Z","shell.execute_reply.started":"2021-11-21T06:59:42.059047Z","shell.execute_reply":"2021-11-21T06:59:42.913118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip uninstall -y segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:43.763745Z","iopub.execute_input":"2021-11-21T04:42:43.764029Z","iopub.status.idle":"2021-11-21T04:42:43.768797Z","shell.execute_reply.started":"2021-11-21T04:42:43.763992Z","shell.execute_reply":"2021-11-21T04:42:43.768092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"markdown","source":"For this example we will use **CamVid** dataset. It is a set of:\n - **train** images + segmentation masks\n - **validation** images + segmentation masks\n - **test** images + segmentation masks\n \nAll images have 320 pixels height and 480 pixels width.\nFor more inforamtion about dataset visit http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/.","metadata":{}},{"cell_type":"code","source":"# import os\n# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nimport torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport shutil\nimport sklearn\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\n\nfrom distutils.dir_util import copy_tree","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:43.77069Z","iopub.execute_input":"2021-11-21T04:42:43.771299Z","iopub.status.idle":"2021-11-21T04:42:46.002987Z","shell.execute_reply.started":"2021-11-21T04:42:43.77126Z","shell.execute_reply":"2021-11-21T04:42:46.002195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:46.005276Z","iopub.execute_input":"2021-11-21T04:42:46.005544Z","iopub.status.idle":"2021-11-21T04:42:46.010317Z","shell.execute_reply.started":"2021-11-21T04:42:46.00551Z","shell.execute_reply":"2021-11-21T04:42:46.008798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '../input/agrocodetransformed/transformed'\nNB_CLASSES = 4\nMAX_N_SAMPLES = 10 ** 9\nids = os.listdir(DATA_DIR)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:46.011839Z","iopub.execute_input":"2021-11-21T04:42:46.012105Z","iopub.status.idle":"2021-11-21T04:42:46.066493Z","shell.execute_reply.started":"2021-11-21T04:42:46.012065Z","shell.execute_reply":"2021-11-21T04:42:46.065785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ids, val_ids = train_test_split(ids, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:46.06777Z","iopub.execute_input":"2021-11-21T04:42:46.068037Z","iopub.status.idle":"2021-11-21T04:42:46.074489Z","shell.execute_reply.started":"2021-11-21T04:42:46.068004Z","shell.execute_reply":"2021-11-21T04:42:46.073647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_ids), len(val_ids)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:46.076208Z","iopub.execute_input":"2021-11-21T04:42:46.0765Z","iopub.status.idle":"2021-11-21T04:42:46.089271Z","shell.execute_reply.started":"2021-11-21T04:42:46.076463Z","shell.execute_reply":"2021-11-21T04:42:46.088477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir train\n!mkdir val\n!mkdir train_labels\n!mkdir val_labels","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:46.090787Z","iopub.execute_input":"2021-11-21T04:42:46.091084Z","iopub.status.idle":"2021-11-21T04:42:49.169207Z","shell.execute_reply.started":"2021-11-21T04:42:46.091045Z","shell.execute_reply":"2021-11-21T04:42:49.168179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sample_id in train_ids:\n    img_pth = glob(os.path.join(DATA_DIR, sample_id, \"images\", \"*\"))[0]\n    shutil.copy(img_pth, f\"train\")\n    \n    label_dir_pth = os.path.join(DATA_DIR, sample_id, \"masks\")\n    copy_tree(label_dir_pth, f\"train_labels/{sample_id}/\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:42:49.171131Z","iopub.execute_input":"2021-11-21T04:42:49.171412Z","iopub.status.idle":"2021-11-21T04:43:17.158139Z","shell.execute_reply.started":"2021-11-21T04:42:49.171364Z","shell.execute_reply":"2021-11-21T04:43:17.157386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sample_id in val_ids:\n    img_pth = glob(os.path.join(DATA_DIR, sample_id, \"images\", \"*\"))[0]\n    shutil.copy(img_pth, f\"val\")\n    \n    label_dir_pth = os.path.join(DATA_DIR, sample_id, \"masks\")\n    copy_tree(label_dir_pth, f\"val_labels/{sample_id}/\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:17.161969Z","iopub.execute_input":"2021-11-21T04:43:17.162191Z","iopub.status.idle":"2021-11-21T04:43:22.986625Z","shell.execute_reply.started":"2021-11-21T04:43:17.162159Z","shell.execute_reply":"2021-11-21T04:43:22.985852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_dir = \"train\"\ny_train_dir = \"train_labels\"\n\nx_val_dir = \"val\"\ny_val_dir = \"val_labels\"","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:22.988002Z","iopub.execute_input":"2021-11-21T04:43:22.988276Z","iopub.status.idle":"2021-11-21T04:43:22.993722Z","shell.execute_reply.started":"2021-11-21T04:43:22.988239Z","shell.execute_reply":"2021-11-21T04:43:22.992969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:22.995499Z","iopub.execute_input":"2021-11-21T04:43:22.996265Z","iopub.status.idle":"2021-11-21T04:43:23.004428Z","shell.execute_reply.started":"2021-11-21T04:43:22.99622Z","shell.execute_reply":"2021-11-21T04:43:23.003673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:23.006015Z","iopub.execute_input":"2021-11-21T04:43:23.006351Z","iopub.status.idle":"2021-11-21T04:43:23.01279Z","shell.execute_reply.started":"2021-11-21T04:43:23.006313Z","shell.execute_reply":"2021-11-21T04:43:23.012022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(BaseDataset):\n    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n#     CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n#                'tree', 'signsymbol', 'fence', 'car', \n#                'pedestrian', 'bicyclist', 'unlabelled']\n    \n    def __init__(\n            self, \n            images_dir, \n            masks_dir\n    ):\n        preprocessing_fn = get_preprocessing(smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS))\n        self.images_files = glob(os.path.join(images_dir, \"*\"))[:MAX_N_SAMPLES]\n        self.masks_dir = masks_dir\n        self.augmentation = augmentation\n    \n        self.images = []\n        self.masks = []\n        \n        for i in range(len(self)):\n            print(f\"enter dataset with index {i}\")\n            image_pth = self.images_files[i]\n            obj_id = image_pth.split(\"/\")[-1].split(\".\")[0]\n\n            image = cv2.imread(image_pth)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, dsize=(512, 512))\n\n    #         mask = cv2.imread(self.masks_fps[i], 0)\n            mask = np.zeros((512, 512, NB_CLASSES), dtype=float)\n            for class_idx in range(NB_CLASSES):\n                # class_idx in annotations starts with 1, so do + 1\n                mask_pthes = glob(os.path.join(self.masks_dir, obj_id, f\"*_class_{class_idx + 1}.png\"))\n                for mask_pth in mask_pthes:\n                    curr_msk = cv2.resize(cv2.imread(mask_pth), dsize=(512, 512)).mean(axis=2) > 0\n                    mask[:, :, class_idx][curr_msk] = 1\n                    \n                sample = preprocessing_fn(image=image, mask=mask)\n                image, mask = sample['image'], sample['mask']\n                image = image.transpose(1, 2, 0).astype(\"float32\")\n                \n            image = image.transpose(2, 0, 1).astype('float32')\n\n            print(\"image shape\", image.shape, \"mask shape\", mask.shape)\n            self.images.append(image)\n            self.masks.append(mask)\n        \n    \n    def __getitem__(self, i: int):\n        \n        img, mask = self.images[i], self.masks[i]\n        print(\"img shape\", img.shape, \"mask shape\", mask.shape)\n        return img, mask\n        \n    def __len__(self):\n        return len(self.images_files)\n\nimport albumentations as albu","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:23.014279Z","iopub.execute_input":"2021-11-21T04:43:23.014613Z","iopub.status.idle":"2021-11-21T04:43:24.195417Z","shell.execute_reply.started":"2021-11-21T04:43:23.014569Z","shell.execute_reply":"2021-11-21T04:43:24.194663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:24.196924Z","iopub.execute_input":"2021-11-21T04:43:24.19719Z","iopub.status.idle":"2021-11-21T04:43:24.204602Z","shell.execute_reply.started":"2021-11-21T04:43:24.197154Z","shell.execute_reply":"2021-11-21T04:43:24.20384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create model and train","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:24.206098Z","iopub.execute_input":"2021-11-21T04:43:24.20637Z","iopub.status.idle":"2021-11-21T04:43:30.284501Z","shell.execute_reply.started":"2021-11-21T04:43:24.206336Z","shell.execute_reply":"2021-11-21T04:43:30.283638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENCODER = 'se_resnext50_32x4d'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = [\"berry\", \"leaf\", \"stem\", \"flower\"]\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\nDEVICE = 'cuda'\n\n# create segmentation model with pretrained encoder\nmodel = smp.FPN(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:43:30.286175Z","iopub.execute_input":"2021-11-21T04:43:30.286436Z","iopub.status.idle":"2021-11-21T04:48:43.361887Z","shell.execute_reply.started":"2021-11-21T04:43:30.286401Z","shell.execute_reply":"2021-11-21T04:48:43.361118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(\n    x_train_dir, \n    y_train_dir, \n    preprocessing=get_preprocessing(preprocessing_fn)\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=12)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-21T04:49:25.171189Z","iopub.execute_input":"2021-11-21T04:49:25.171646Z","iopub.status.idle":"2021-11-21T04:52:44.544335Z","shell.execute_reply.started":"2021-11-21T04:49:25.171604Z","shell.execute_reply":"2021-11-21T04:52:44.543494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = train_dataset[0]\nplt.imshow(x.transpose((1, 2, 0)).astype(\"float\"))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:52:44.545587Z","iopub.execute_input":"2021-11-21T04:52:44.54621Z","iopub.status.idle":"2021-11-21T04:52:44.83856Z","shell.execute_reply.started":"2021-11-21T04:52:44.546172Z","shell.execute_reply":"2021-11-21T04:52:44.837866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n\nloss = smp.utils.losses.DiceLoss()\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:52:44.839599Z","iopub.execute_input":"2021-11-21T04:52:44.839848Z","iopub.status.idle":"2021-11-21T04:52:44.847717Z","shell.execute_reply.started":"2021-11-21T04:52:44.839795Z","shell.execute_reply":"2021-11-21T04:52:44.846653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:52:44.84898Z","iopub.execute_input":"2021-11-21T04:52:44.849367Z","iopub.status.idle":"2021-11-21T04:52:44.907104Z","shell.execute_reply.started":"2021-11-21T04:52:44.849328Z","shell.execute_reply":"2021-11-21T04:52:44.906209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:52:44.908716Z","iopub.execute_input":"2021-11-21T04:52:44.909022Z","iopub.status.idle":"2021-11-21T04:52:49.289498Z","shell.execute_reply.started":"2021-11-21T04:52:44.908984Z","shell.execute_reply":"2021-11-21T04:52:49.288757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create epoch runners \n# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=device,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=device,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:52:49.29097Z","iopub.execute_input":"2021-11-21T04:52:49.291213Z","iopub.status.idle":"2021-11-21T04:52:49.305762Z","shell.execute_reply.started":"2021-11-21T04:52:49.291179Z","shell.execute_reply":"2021-11-21T04:52:49.30515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model for 40 epochs\n\nmax_score = 0\n\nfor i in range(0, 40):\n    \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    valid_logs = valid_epoch.run(valid_loader)\n    \n    # do something (save model, change lr, etc.)\n    if max_score < valid_logs['iou_score']:\n        max_score = valid_logs['iou_score']\n        torch.save(model.state_dict(), './best_model.pth')\n        print('Model saved!')\n        \n    if i == 25:\n        optimizer.param_groups[0]['lr'] = 1e-5\n        print('Decrease decoder learning rate to 1e-5!')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T04:52:49.307123Z","iopub.execute_input":"2021-11-21T04:52:49.307371Z","iopub.status.idle":"2021-11-21T05:02:44.128648Z","shell.execute_reply.started":"2021-11-21T04:52:49.307338Z","shell.execute_reply":"2021-11-21T05:02:44.127361Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test best saved model","metadata":{}},{"cell_type":"code","source":"# load best saved checkpoint\nmodel.load_state_dict(torch.load('./best_model.pth'))\nbest_model = model","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:02:44.130367Z","iopub.execute_input":"2021-11-21T05:02:44.130908Z","iopub.status.idle":"2021-11-21T05:02:44.26741Z","shell.execute_reply.started":"2021-11-21T05:02:44.130865Z","shell.execute_reply":"2021-11-21T05:02:44.266657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model on test set\ntest_epoch = smp.utils.train.ValidEpoch(\n    model=best_model,\n    loss=loss,\n    metrics=metrics,\n    device=DEVICE,\n)\n\nlogs = test_epoch.run(valid_loader)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:02:44.272242Z","iopub.execute_input":"2021-11-21T05:02:44.272607Z","iopub.status.idle":"2021-11-21T05:02:46.071537Z","shell.execute_reply.started":"2021-11-21T05:02:44.272576Z","shell.execute_reply":"2021-11-21T05:02:46.070748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize predictions","metadata":{}},{"cell_type":"code","source":"from skimage import io\nfrom skimage import color\nfrom skimage import segmentation","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:02:46.074786Z","iopub.execute_input":"2021-11-21T05:02:46.075037Z","iopub.status.idle":"2021-11-21T05:02:46.23712Z","shell.execute_reply.started":"2021-11-21T05:02:46.075007Z","shell.execute_reply":"2021-11-21T05:02:46.236325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_idx = 3","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:07:15.965301Z","iopub.execute_input":"2021-11-21T05:07:15.965557Z","iopub.status.idle":"2021-11-21T05:07:15.96915Z","shell.execute_reply.started":"2021-11-21T05:07:15.965528Z","shell.execute_reply":"2021-11-21T05:07:15.96838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    n = np.random.choice(len(valid_dataset))\n    \n#     image_vis = test_dataset_vis[n][0].astype('uint8')\n    image, gt_mask = valid_dataset[n]\n    \n    gt_mask = gt_mask.squeeze()\n    \n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    pr_mask = best_model.predict(x_tensor)\n    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n    \n    image = np.transpose(image, (1, 2, 0))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    print(pr_mask.shape, gt_mask.shape)\n    visualize(\n        image=image, \n        ground_truth_mask=gt_mask[label_idx], \n        predicted_mask=pr_mask[label_idx],\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:07:16.254122Z","iopub.execute_input":"2021-11-21T05:07:16.254578Z","iopub.status.idle":"2021-11-21T05:07:17.643419Z","shell.execute_reply.started":"2021-11-21T05:07:16.25454Z","shell.execute_reply":"2021-11-21T05:07:17.64262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import draw_segmentation_masks","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:02:47.889083Z","iopub.execute_input":"2021-11-21T05:02:47.889423Z","iopub.status.idle":"2021-11-21T05:02:47.893498Z","shell.execute_reply.started":"2021-11-21T05:02:47.88939Z","shell.execute_reply":"2021-11-21T05:02:47.892728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_mask.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:07:36.954365Z","iopub.execute_input":"2021-11-21T05:07:36.954624Z","iopub.status.idle":"2021-11-21T05:07:36.960154Z","shell.execute_reply.started":"2021-11-21T05:07:36.954595Z","shell.execute_reply":"2021-11-21T05:07:36.959479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:14:21.62342Z","iopub.execute_input":"2021-11-21T05:14:21.623907Z","iopub.status.idle":"2021-11-21T05:14:21.657637Z","shell.execute_reply.started":"2021-11-21T05:14:21.623857Z","shell.execute_reply":"2021-11-21T05:14:21.656717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimage = cv2.imread(\"./val/37786E52-F57E-4777-A9E8-D94CBFE89EAD_1_105_c.jpeg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\npreprocessing_fn = get_preprocessing(smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS))\nx, _ = preproc_image(image)\n\nx_tensor = torch.from_numpy(x).to(DEVICE).unsqueeze(0)\npr_mask = best_model.predict(x_tensor)\npr_mask = (pr_mask.squeeze().cpu().numpy().round())\n","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:38:27.774568Z","iopub.execute_input":"2021-11-21T05:38:27.774857Z","iopub.status.idle":"2021-11-21T05:38:27.846115Z","shell.execute_reply.started":"2021-11-21T05:38:27.774821Z","shell.execute_reply":"2021-11-21T05:38:27.845369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_tensor = torch.tensor(np.transpose(image, (2, 0, 1)), dtype=torch.uint8)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:38:27.977568Z","iopub.execute_input":"2021-11-21T05:38:27.977861Z","iopub.status.idle":"2021-11-21T05:38:27.982529Z","shell.execute_reply.started":"2021-11-21T05:38:27.977828Z","shell.execute_reply":"2021-11-21T05:38:27.981846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_masks = [cv2.resize(pr_mask[idx], dsize=(img_tensor.shape[2], img_tensor.shape[1])) for idx in range(4)]","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:38:28.558029Z","iopub.execute_input":"2021-11-21T05:38:28.558496Z","iopub.status.idle":"2021-11-21T05:38:28.569525Z","shell.execute_reply.started":"2021-11-21T05:38:28.558462Z","shell.execute_reply":"2021-11-21T05:38:28.56878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pr_mask = np.stack(pr_masks)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:38:28.770072Z","iopub.execute_input":"2021-11-21T05:38:28.770587Z","iopub.status.idle":"2021-11-21T05:38:28.776702Z","shell.execute_reply.started":"2021-11-21T05:38:28.770549Z","shell.execute_reply":"2021-11-21T05:38:28.776002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(img_tensor.shape, pr_mask.shape)\n\nsegm_img = draw_segmentation_masks(img_tensor, torch.tensor(pr_mask, dtype=torch.bool), alpha=0.3, colors=[\"blue\", \"red\", \"purple\", \"green\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:40:45.411033Z","iopub.execute_input":"2021-11-21T05:40:45.411523Z","iopub.status.idle":"2021-11-21T05:40:45.444928Z","shell.execute_reply.started":"2021-11-21T05:40:45.411486Z","shell.execute_reply":"2021-11-21T05:40:45.444138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nplt.imshow(np.transpose(segm_img.numpy(), (1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:40:47.433615Z","iopub.execute_input":"2021-11-21T05:40:47.434304Z","iopub.status.idle":"2021-11-21T05:40:48.120578Z","shell.execute_reply.started":"2021-11-21T05:40:47.434264Z","shell.execute_reply":"2021-11-21T05:40:48.119736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DeployedSegmentation:\n    def __init__(self, weights_pth: str):\n        ENCODER = 'se_resnext50_32x4d'\n        ENCODER_WEIGHTS = 'imagenet'\n        self.CLASSES = [\"berry\", \"leaf\", \"stem\", \"flower\"]\n        ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n        DEVICE = 'cuda'\n\n        # create segmentation model with pretrained encoder\n        self.model = smp.FPN(\n            encoder_name=ENCODER, \n            encoder_weights=ENCODER_WEIGHTS, \n            classes=len(self.CLASSES), \n            activation=ACTIVATION,\n        )\n        \n    def get_preds(self, img_pth: str) -> torch.Tensor:\n            \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(pr_mask[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T06:31:10.877211Z","iopub.execute_input":"2021-11-21T06:31:10.877465Z","iopub.status.idle":"2021-11-21T06:31:11.173488Z","shell.execute_reply.started":"2021-11-21T06:31:10.877437Z","shell.execute_reply":"2021-11-21T06:31:11.172647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = torch.zeros(10, dtype=torch.bool)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:08:55.108518Z","iopub.execute_input":"2021-11-21T05:08:55.108779Z","iopub.status.idle":"2021-11-21T05:08:55.112597Z","shell.execute_reply.started":"2021-11-21T05:08:55.108748Z","shell.execute_reply":"2021-11-21T05:08:55.11193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:10:03.868944Z","iopub.execute_input":"2021-11-21T05:10:03.869467Z","iopub.status.idle":"2021-11-21T05:10:03.882751Z","shell.execute_reply.started":"2021-11-21T05:10:03.869429Z","shell.execute_reply":"2021-11-21T05:10:03.881786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls val","metadata":{"execution":{"iopub.status.busy":"2021-11-21T05:37:52.708789Z","iopub.execute_input":"2021-11-21T05:37:52.709159Z","iopub.status.idle":"2021-11-21T05:37:53.60891Z","shell.execute_reply.started":"2021-11-21T05:37:52.709109Z","shell.execute_reply":"2021-11-21T05:37:53.608054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}